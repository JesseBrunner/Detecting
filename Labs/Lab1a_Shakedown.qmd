---
title: "Lab 1: The R Shakedown"
author: "Jesse Brunner"
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    embed-resources: true
    code-fold: false
---

## Our goals and rationalle

Much of our work in this class will involve working with data. Whether the raw data from our studies or samples from prior or posterior distributions (trust me, this will make sense), it is all data. Often, however, the data we have access to is not in the format or organized or summarized how we need it. While I assume we have have used R to varying extents, I suspect we have done so in different ecosystems (e.g., base R vs. Tidyverse) and with different tools and workflows. Sometimes we may have even just "borrowed" code that "seemed to work" without really knowing what we were doing. 

Given all of this, my goals for this lab are to:

  * Develop a common workflow
  * Make a habit of knowing what we are doing or working with
  * Ensuring we have common tools so we can be on the same page
  * Introduce some base R functions we will see used frequently in the text

On this last point, we are going to try to lean on the tools that McElreath uses, that is mostly base R functions, so that we can easily compare our work with his. I tend to use Tidyverse tools, myself, and there will be room for using whatever tools you like once you know what you are doing with them, but there is real value to _knowing_ what he is doing in his examples because you are doing the same things. 

::: {.callout-important}
One of the best things about modern statistical computing is the widespread availability and use of scripts or workbooks. That latter, which includes text explanations of what it is you are doing, code to do it, and, once you run the file, the output, all seamlessly knitted together. In R/Rstudio that available with Quarto documents (`qmd` files). Eventually you might write entire scientific papers as Quarto documents, create presentations in Quarto, or even write a book using tools like Quarto! For this class our goal is more modest: we'll just use these files to keep our text thoughts (context, goals, caveats, etc) and code + output in one place. So buckle up, our labs will all be completed in Quarto documents^[Indeed, the labs and even most presentations are _written_ as Quarto files, so you can look under the hood of anything!]. 

For now, just go to File / New File / Quarto Document and follow the prompts to create a file for this lab. Save often, but whenever you want, hit the "Render" button at the top of the window and it will run the code in a new instance of R and create an output file (html, by default, but other format are available). When you want to share things with me, please be sure you use Quarto documents.
:::

# Example 1: Small mammals
One of my former students conducted a capture-mark-recapture study at Smoot Hill. You can find the data set [online, here](https://s3.wp.wsu.edu/uploads/sites/1097/2019/08/Eilers.csv). In general, we need to: 

  1.  load the data, 
  2.  check the data, 
  3.  make any adjustments that might be needed^[We should fix mistakes in our data sets, but any modifications, calculations, or conversions should be done in code.], 
  4.  calculate summaries and make plots to ensure we understand what we are dealing with, and 
  5.  re-organize, summarize, or manipulate the data into the format we need, repeating prior steps as needed.
  
Indeed, this will be our general workflow for dealing with data because it emphasizes knowing what the heck we have at every step of the process. In our specific case, let's imagine that we want to determine the average and distribution of times captured for each species. 

## Load the data
```{r}
smammals <- read.csv("https://s3.wp.wsu.edu/uploads/sites/1097/2019/08/Eilers.csv") 
```

This isn't always easy, as you likely know. Usually the issues are with formatting and hidden gremlins in Excel. 

## Check the data

Let's take a look at what we just loaded up.
```{r}
head(smammals)
tail(smammals) # always check the bottom, too!
```

We can see that each row represents one animal capture. 

Now, let's see how our data are _represented_ in R at present.
```{r}
summary(smammals)
str(smammals)
```

Nothing seems terribly amiss, yet. In both of these ways we can see that every variable that is not obviously a number is treated as a character. We might want to change that using, for instance, `factor()` or `as.Date()`^[or `lubridate::ymd()` in the Tidyverse]. Dates are the worst to deal with. Just a heads up that whenever you have dates in Excel you _must_ check to see that R has interpreted things correctly. I can't tell you how often my otherwise perfect code has been foiled by input that wasn't actually the date I had thought it was. Anyway, for now, we can ignore everything but one: while `Tag`, referring to the ear tag, is really a categorical variable, R thinks it is an integer and will try to do math with it. 

## Adjust the variables

Let's make `Tag` a factor. We really don't care what about the order, just that they aren't treated like numbers, so this is easy:
```{r}
smammals$Tag <- factor(smammals$Tag)
```
And for fun, let's make `Date` an actual date. 
```{r}
smammals$Date <- as.Date(smammals$Date, format = "%m/%d/%Y")
```
where `%m` means month numerically (`%b` is the three letter month code),  `%d` is the day of the month, and `%Y` is the four digit year (`%y` is the two digit year), while the slashes in between represent the actual dividers between date components. Be sure to check the formatting of these variables to ensure this stuff worked!

## Look at summaries and plots to understand data

We have already checked the distributions of the values of lengths and weights using the `summary()` function, but we should still plot our data to make sure that combinations of variables make sense. Trust me, things get shifted, relationships are the opposite of what you expect, and other gremlins exist. If you don't check, you won't know there is a problem until it is too late!

Let's simply plot weight by body length for each species to make sure these variables generally go up together as expected. 

```{r}
plot(Weight ~ Body, col = factor(Species), data = smammals)
```

Full disclosure, I hate plotting in base R! I can't even figure out how to add a meaningful legend anymore. This is much simpler with `ggplot2`, if you know how that works:
```{r}
library(ggplot2)
ggplot(data=smammals, 
       aes(x=Body, y = Weight, 
           color = Species, shape = Species)) + 
  geom_point()
```
I'm fine with you using whatever plotting functions you use. (McElreath often uses his own because why not?)

Anyway, we are starting to see some issues showing up in both graphs. First, it looks like only PEMA and two ZAPR are showing up on both plots. Is that right? Second, why does the y-axis go all the way to 70 when the largest weight seems to be about 30? Indeed, if we look at our summaries from above, the maximum weight is 70, so why isn't this plotting? Time for some data inspection!

```{r}
summary(smammals[smammals$Species == "TAAM", ])
```

First, be sure you understood what's happening here (i.e., how I'm subsetting the `smammals` data set by those where the condition is true and then getting a summary of this reduced data set^[And yes, this would be much easier to follow in the Tidyverse! But not all things are.]). The key to understanding is taking things in steps. What does the `smammals$Species == "TAAM"` command do? What does it mean if I insert it in brackets after the name of the data set? Figure it out before jumping to the next part of it. (And ask questions when they arise!)

We could do something similar using the `tapply()` function:
```{r}
tapply(X = smammals$Body, 
       INDEX = smammals$Species, 
       FUN = summary)
```

I like to think of this function as the "table" apply function because it first organizes our data like a data table, according to whatever index we've given it (here, `Species`), and then applies the function of our choice (here, `summary()`). Or, more specifically, it applies the function specified by `FUN` to the vector (or similar) specified by `X`, which is split into groups according to the variable specified by `INDEX`. We will see and use several `apply` functions in this class (mostly `apply()`, which we will see below, and `sapply()`). They are _not_ always very intuitive, but they are very powerful and McElreath uses them frequently, so it behooves us to get used to them^[Also note, in this case it would be easy to accomplish the same thing in the Tidyverse, but there are several cases we'll see where it is _not_ easy or transparent to do so. So let's stick with base R stuff.]

Anyway, we can see that we were missing data on `Body` length for all but PEMA and ZAPR, hence why they didn't show up. Since we're not really interested in this, we can move on, but it does highlight why _looking_ at your data is important. 

## Re-organize, summarize, or manipulate

So our real question was how many captures individual animals had and how that varied by species. This is a bit complicated because the data are structured such that each row is a separate capture. There is no variable that represents how many times an individual (represented by `Tag`) was captured other than the number of rows it occurs over. In other words, we need to re-organize or summarize our data. 

Let's see if we can't use that same trick with `tapply()`
```{r}
tapply(X = smammals$Species, 
       INDEX = smammals$Tag, 
       FUN = length)
```
Well, that didn't quite get us where we wanted. We have the number of rows per individual (i.e., `Tag`), but we lost their species identity in the process. (Note, too, that it didn't matter which vector we used for `X` since we were just finding the length.)

What if we give two grouping or `INDEX` variables?
```{r}
tapply(X = smammals$Species, 
       INDEX = list(smammals$Tag, smammals$Species), 
       FUN = length)
```

That's sort of better. We could convert this into a data frame and then calculate summaries from it.

```{r}
bySpp <- tapply(X = smammals$Species, 
                INDEX = list(smammals$Tag, smammals$Species), 
                FUN = length)

summary(bySpp)
```

Perhaps this is enough for us, but I think a plot of the data makes these summary statistics much easier to grasp. So let's plot a histogram of the number of captures per individual for each species.

```{r}
# Start with TAAM, which had most values
hist(bySpp[,2], breaks = 0:8, ylim = c(0, 45)) 
hist(bySpp[,1], add = T, border = "red", col = NULL)
hist(bySpp[,3], add = T, border = "blue", col = NULL)
hist(bySpp[,4], add = T, border = "green", col = NULL)
```

Or again, I find this so much easier if we just use `ggplot`:
```{r}
library(tidyverse) # Need this for the pivot_longer
data.frame(bySpp) %>% 
  # need to convert from one col per spp -> spp col + capture col
  pivot_longer(names_to = "Species", 
               values_to = "Captures", 
               cols = everything()) %>% 
  ggplot(., aes(x=Captures, color = Species) ) + 
  geom_histogram(position = "identity", fill = NA, breaks = 0:8, na.rm=TRUE) + 
  theme_bw()
```

So I think that is a pretty reasonable summary of the data in terms of how often individuals of each species are captured.

## An aside: the `apply()` function
Now in the spirit of preparing us for the functions we will see in the near-ish future, and since we just used the `tapply()` function, let me illustrate how might use the `apply()` function. This function simply, well, _applies_ a function to each row or column (i.e., a margin) of an array or matrix. Margins are represented by `1` for rows or `2` for columns. Let's try it. 

```{r}
apply(bySpp, 
      MARGIN = 2, 
      FUN = summary)
```

So, it gives us the same basic information (with different formatting) as before. But we can use it more broadly, too.

For instance, what do you think this will do? Seriously, take a second and think before running it.
```{r}
#| output: false
apply(smammals, 
      MARGIN = 2, 
      FUN = min, na.rm=T)
```


.  
.  
.  

Notice that the data frame, `smammals` was coerced into a matrix which can only accommodate a single data type. In this case the lowest common denominator was `character`, so all of the columns became character types. 

Try changing the the function or the margin to see if you understand how this works. Bonus points for trying it on one of your own data sets.

Anyway, here in this context this function is a bit less useful than `tapply()`, but it _will_ be used a lot in this course. So consider this your gentle introduction to `apply()`. 



# Example 2: Weather data from loggers

As is often the case, we happen to have weather data from nearby data loggers in a separate data set from the trapping data. The `climate` data set can be found [here](https://s3.wp.wsu.edu/uploads/sites/1097/2019/09/Climate.csv). 

## Load and check the data
Load them in---don't forget to to format the date and look at the data. 
```{r}
climate <- read.csv("https://s3.wp.wsu.edu/uploads/sites/1097/2019/09/Climate.csv")
head(climate)
tail(climate)
summary(climate)
```
Notice the columns for temperature (degrees C) and relative humidity (%). It looks like we get data every hour, so there should be ~24 readings per day. Finally, there are blessedly no missing data or apparent wonkiness---though note that `Date` and `Time` are still treated as character, so there could be gremlins lurking there. 

See, too, that there are multiple data loggers. 
```{r}
unique(climate$LoggerID)
```

Now imagine that what we need to do is calculate the average daily temperature across all of the loggers for each day. (And a heads up, we may not have the same number of loggers operating on any given day. Does that matter?) So here's what we need to do:

1. Correct the `Date` column so it is treated as a date
2. Calculate the average temperature for each loggers for each day
3. Calculate the average daily temperature across the active loggers

Before proceeding, stare off into space for a second and think about how you would approach this. What would your strategy be? Do you see a way forward?

.  
.  
.  

## Adjust the variable

This should be the easy-ish part. Look above to see how we changed a vector of dates treated as characters to a vector of dates treated as dates. I won't show you the code---you need to try it---but here is the result:

```{r}
#| echo: false
climate$Date <- as.Date(climate$Date, format = "%m/%d/%Y")
```

```{r}
# INSERT CODE TO MAKE COLUMN "Date" A DATE
summary(climate$Date)
```

So now `Date` is a real date and we can see that our data set goes from June 1 to August 31. That makes sense, so no (obvious) gremlins here!

::: {.callout-warning}
Times of day are usually not dealt with independently of date^[There are probably some specific time-of-day formats, but they are special use cases.], which has caused me no shortage of frustration in the past. So let me just show you how to convert both the date and the time to a date-time column. In essence, we are going to paste together the two vectors with `paste()` and the provide the format of that date-time agglomeration.  
```{r}
climate$DT <- strptime(paste(climate$Date, climate$Time), 
                       format = "%Y-%m-%d %H:%M:%S")
head(climate)
str(climate)
```

Getting sensible values out of this, as opposed to just formatting the output so it shows you what you like (see the help files), is...difficult. If you often need to do this I would _strongly_ recommend keeping separate columns for hours, or maybe something like minutes from midnight that you can more easily do math on. That said, methods do exist and you can use them, but it won't (initially) be easy. Phew! Go that off my chest. Now we can move on.
:::

## Calculate summaries

Again, we first want to calculate the mean for each logger on each day. We can use `tapply()` again. Here the thing we want our function applied to is the temperature and we want to first organize these temperature data by logger and date (or vice versa... try reversing the arguments in the list and see what happens).  
```{r}
daily_logger <- tapply(X = climate$Temperature, 
                       INDEX = list(climate$Date, climate$LoggerID), 
                       FUN = mean)

head(daily_logger)
```

Please notice the structure of the data before proceeding. Each row is a date and each column a logger. There are a lot of `NA`s simply because that particular logger was not deployed on that particular day. 

Now, if I were to suggest you use `apply()` to apply the `mean()` function to this new matrix, `daily_logger`, which margin would you want it to be applied to? That is, should we take the average of each row or each column? (And if we had reverse the arguments to `INDEX`, above, would we need to change this?)

OK, here goes...
```{r}
daily <- apply(X=daily_logger, 
               MARGIN = 1, 
               FUN = mean, na.rm=TRUE)

head(daily)
```

Before moving on, make sure you understand what those functions _do_. What happens if we didn't include the `na.rm=TRUE` in the command? Why?

Finally, be sure you see whether this is what we actually wanted to produce. 

## Plot the data

The only problem with our data as it is formatted is that it's just a vector of values (mean temperatures) with no dates. I mean, they are there, but in the form of rownames. So we can plot these values:
```{r}
plot(daily)
```

but they are plotted against the index of their position in the vector, not by date. We can clean this up a bit:
```{r}
daily <- data.frame(daily)
head(daily)

# use the rownames to make a date column
daily$Date = as.Date(rownames(daily))
str(daily)

# change the name of the temperature variable
colnames(daily)[1] <- "MeanTemp" # see what I'm doing?
```
And then we can plot it nicer
```{r}
plot(MeanTemp ~ Date, data = daily)
# Or
ggplot(data = daily, aes(x=Date, y = MeanTemp)) + 
  geom_line() + geom_point() + 
  scale_x_date(minor_breaks = "week") + 
  scale_y_continuous("Mean daily temperature (°C)", 
                     minor_breaks = 10:30)
```

# Example 3: vapor pressure deficit & functions


The `climate` data set has temperature and humidity, but not vapor pressure deficit (VPD), a measure of the drying potential of the conditions^["The strain under which an organism is placed in maintaining a water balance during temperature changes is much more clearly shown by noting the vapor pressure deficit than by recording the relative humidity."Anderson, D. B. 1936. Relative humidity or vapor pressure deficit. Ecology 17, no. 2: 277–282]. That, however, can be calculated as:
$$
0.6108 \times \exp \left(17.27 \times \frac{T}{ (T + 237.3)}\right) \times \left(1-\frac{RH}{100}\right).
$$

Your final tasks are to use the tools we've seen already to calculate and plot the VPD over the site across all of the loggers, and plot the maximum VPD recorded by each logger as a quality control check. I will not show you the code---just the results---but I will give you one last tool to use.

## An aside on functions
We could calculate VPD by hand, each time we needed, or we could make a function to do so and use that function whenever we needed to calculate VPD. That sounds easier, long run. So, let us learn about writing functions. 

A function is written as:
```{r}
#| eval: FALSE
name_of_fxn <- function(argument1, argument2, ...){
  #Stuff to do.
  return()
}
```

The arguments in the parentheses of `function()` are what the function uses^[Functions often have sensible default values to use when the argument isn't specified. You provide these in the function definition, e.g., as `argument1 = 10, argument2 = TRUE`.]. The thing in the parentheses of `return()` is what is given back. A function can only be one thing, but that thing can be a value, vector, list, and so on. So it can include a lot of stuff. Or nothing. So, anyway, a function that calculated the mean of two numbers could be written as:
```{r}
mean2 <- function(x1, x2){
  m <- (x1+x2)/2
  return(m)
}
mean2(x1=12, x2=35)
```

One important aspect of variable names within functions is that the names are "local." This means that while the `mean2()` function uses two variables called `x1` and `x2`,  those are _internal_ to the function. If you had a variable named `x1` elsewhere in your environment, `mean2()` would not know to use that (unless told) and whatever happens in `mean2()` would not change its value.
```{r}
x1 <- 1000
mean2(x1=5, x2=0)
x1 # unchanged
mean2(x1=5, x2=x1) # = (5+1000)/2
x1 # still unchanged
```

This behavior can be a bit confusing, but it is important so that someone using a function does not need to know about how it works inside to avoid conflicts or other unexpected behavior.


So anyway, our VPD function might look like this:
```{r}
vpd <- function(temperature, RH){  
  # temperature in °C, RH is a percent

  # saturation vapor pressure
  es <- 0.6108 * exp(17.27 * temperature / (temperature + 237.3))

  # Actual Vapor Pressure (ea) =
  ea <- (RH / 100) * es

    # Vapor Pressure Deficit in kPa
  return (es- ea)
}

vpd(temperature = 25, RH=80) # should be 0.63356
```

It is not important that you understand the math, but notice that the function is

* easy to read (multiple lines, common indentations) 
* easy to parse (breaks the problem into bite-sized bits)
* well-commented 
* it defines variables and provides units

All of that makes it easier to use and maintain. But now, we want to use it.

# Your assignment

1. Plot mean daily VPD against date (with whatever plotting function you like), like we just did with temperature.

```{r}
#| eval: false
#| echo: false
climate$VPD <- vpd(temperature=climate$Temperature, RH=climate$RH)

 
daily_logger <- tapply(X = climate$VPD, 
                       INDEX = list(climate$Date, 
                                    climate$LoggerID), 
                       FUN = mean)

daily <- apply(X = daily_logger, 
               MARGIN = 1,  
               FUN = mean, na.rm=TRUE)

daily <- data.frame(daily)
 
daily$Date <- as.Date(rownames(daily))
 
colnames(daily)[1] <- "MeanVPD"
```

```{r}
#| eval: false
#| echo: false
ggplot(data = daily, aes(x=Date, y = MeanVPD)) + 
  geom_line() + geom_point() + 
  scale_x_date(minor_breaks = "week") +
  scale_y_continuous("Mean daily vapor pressure deficit (kPa)")
```

2. Plot the maximum VPD measured by each logger.


```{r}
#| eval: false
#| echo: false
logger <- tapply(X = climate$VPD, 
                 INDEX = climate$LoggerID, 
                 FUN = max)
logger <- data.frame(logger)
logger$ID <- rownames(logger)
 
colnames(logger)[1] <- "MaxVPD"
```

```{r}
#| eval: false
#| echo: false
ggplot(data = logger, aes(x=MaxVPD, y = ID)) + 
  geom_point() + 
  scale_x_continuous("Maximum vapor pressure deficit (kPa)") +
  labs(caption = "Not sure why I rotated the figure, but there you go")
```
