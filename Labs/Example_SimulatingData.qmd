---
title: "Example of simulating data"
author: "Jesse Brunner"
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    embed-resources: true
    code-fold: false
execute:
  include: true
---

```{r}
#| include: false
set.seed(101)
```


There are a lot of abstract ideas and seemingly convoluted concepts in this class, and those ideas can slip around or just go _poof_!  Sometimes an example helps clarify things. So let me give you an example of simulating data. 

# The system and question

My former M.S. student, Mitch Le Sage, was interested the role that scavenging invertebrates might play in the transmission and dynamics of _Ranavirus_ epidemics. His general hypothesis was that by removing infectious carcasses, scavengers might reduce transmission rates among amphibian larvae in ponds. 

One question that came out of this research was whether invertebrate scavengers, such as dytiscid beetle larvae, were capable of removing many amphibian carcasses. Eating one carcass in a lab setting is one thing, but what if there were a lot carcasses, as in a die-off? Would the scavengers be able to keep up?

Mitch and an undergraduate in the lab set up an experiment where dytiscid beetle larvae were house in small aquaria with 1, 2, 5, 8, or 10 carcasses of _Ambystoma macrodactylum_ larvae to see how much was consumed in a 48 hour period. Since carcasses are scavenged, rather than consumed whole, he measured the starting and ending mass of the carcasses. 

This sort of experiment falls reasonably well under the heading of "Functional Responses", or perhaps you have heard of the "Holling Disc equations." The history of these is pretty fun^[To better understand how animals might forage, what strategies they would take, etc., he had undergraduate volunteers pick up discs of sand paper (I think... my recollection is imperfect) with tacks or their fingers while blind folded, etc., counting how many they could get in a certain amount of time.  He noticed some interesting empirical patterns, which others then built some equations to describe and even explain; I don't think he was that mathematically inclined. Anyway, they've been hugely influential and useful, including here.], but for our purposes we can just consider two versions of how the foraging (or scavenging) rate increases with the availability of food (or carcasses). 

First, we might simply assume that the foraging rate increases with the density of food. If there are more carcasses in the environment a scavenger will find one to consume more quickly. We could write:

$$
\mu_i = a \times c_i,
$$
meaning that the average or expected amount of carcass mass consumed in a particular aquarium ($\mu_i$) increases linearly with the amount of carcass mass available ($c_i$) at rate $a$. The parameter $a$ is often called the "attack rate." This is called, very creatively, the Type I functional response.

Secondly, we might assume that the rate of carcass removal saturates or levels off simply because a scavenger can only eat carcass so fast. At low carcass densities the scavenging rate is limited by the time it takes to find a carcass, but when carcasses are abundant the rate of removal is instead limited by handling time, $h$. A classic way to frame this so-called Type II functional response^[There are actually a lot of equations that produce more or less similar curves, starting with different assumptions. The Michaelis-Menton equation is one common approach. It just goes to show you that a hypothesis can be represented by multiple models and a model can be consistent with multiple equations.] is:

$$
\mu_i = \frac{a \times c_i}{1+a\times c_i\times h}
$$
We wanted to see which of these models best described carcass removal. We then wanted to extrapolate our findings, including some sense of our uncertainty in the projections, to the pond-level. 

The first step was to simulate data to see how things worked, figure out what we could expect to see, and even tweak the design to better be able to distinguish which of the two models better described our data. 

# Simulating data from the Type I functional response
We'll begin with just the linear model, $\mu_i = ac_i,$ and then repeat the pattern with with the second model. Simulating data seems hard, but honestly, it's just specifying variables, constraints, relationships, and distributions. If that seems like a lot, we'll take it in stages, like you might think about setting up your study. 

## part one: the predictor variable
Let's begin by thinking about the mass of carcasses available to scavengers in our study (which we wrote as $c_i$). Some biological knowledge is helpful. A single larval ambystoma larva might weigh as little as ~1/5th of a gram, although ~1g is more common, so might want to low end of our study design to use $c_i = 0.2g$. What's the upper end? Maybe 10 carcasses, or $c_i = 1g \times 10 = 10g$. So we have the ends of the spectrum. We also knew we had 18 aquaria to use (two broke), so that's how many individual observations we could have. 

Here's one way we might specify the carcass weights available to the scavengers (i.e., the points along the x-axis we'll simulate consumption rates at). 

```{r}
n <- 18 # number of aquaria

carc <- runif(n, min=0.2, max = 10)
carc
```
So these are some made up observations of initial carcass masses. 

There are two things that are quite unrealistic. First, we have a huge level of digits to the right of the decimal place, but that's probably not something to worry about. Second, since we actually added 1, 2, 5, 8, or 10 carcasses in a tank it's probably unrealistic to assume that all carcass masses are equally likely, even though the carcasses varied a bit in size. We could just run with this---it may not matter a lot---or we could more realistically simulate our carcass masses. 

Here, then, is a second way to simulate the carcass masses that is a bit closer to how we wanted to do things:
```{r}
# actual numbers of carcasses used in each replicate
n_carc <- c( rep(1, 5), rep(2, 5), rep(5, 3), rep(8, 3), rep(10, 2) )

# now let's draw random carcass sizes and 
carc <- rnorm(n=18, mean = 1*n_carc, sd = 0.25) # mean of 1g/carc
carc
```

Surprisingly, that actually does a reasonable job of recapitulating the actual carcass masses we ended up with! Anyway, you can see how we might futz with the simulation to try different experimental designs if we wanted to, make the made up data more realistic, etc.

## part two: the relationship

Next, we need to simulate how much of the initial carcass mass is removed. Let's pick an attack rate ($a$ in the Holling disc equation). You may not know much about what this should be, but you can at least specify some basic constraints. It cannot be greater than 1, as that would imply that more carcasses are eaten than available. Also, it must be greater than zero (negative numbers would be like the scavengers are barfing up and thus adding to the pile of carcasses in their aquarium, which is both gross and unrealistic). So we be confident that $1 \geq a \geq 0$. If we thought some more about the math and the system we could probably do better---for instance, only a fraction of the carcass is scavengable; the cartilage is probably not. Anyway, we can come back to this and try other values---that's a key part of this process---but for now let's just say $a=1/2$. 

```{r}
a <- 0.5 # attack rate in g/48h
```

In this case, simulating the expected amount of carcass eaten in each aquarium, $i$, is really simple: $\mu_i = a \times c_i$:

```{r}
mu <- 0.5*carc # the expected or mean amount of carcass eaten.
```

Let's plot our new "data". 
```{r}
plot(mu ~ carc)
```

And...it's a line. I guess that shouldn't be surprising, right? We just specified that it would be! And check out the spacing along the x-axis. That looks right. 

But we want _data_, or "observations," which should not follow exactly along this predicted or expected line, right? I mean, we'd expect a bit of noise both from measuring carcass remains^[Not a pretty sight!] and from difference in, among other things, the appetite of a dytiscid beetle larva, how good or bad it is at finding carcasses, and vagaries of where the carcasses end up relative to the beetle larva. Thus, we want to simulate _observations_ from our _expectations_. 

## part three: the response variable

This is where a lot of us get stuck. How do we go from expectation to (fake) data? Thankfully, we really just need to specify the _distribution_ of data given expectations. Again, there are lots of possibilities, potentially an overwhelming number of options, but we can start by specifying some constraints. First, we know that the remaining mass of carcasses is going to be positive (>0g) and continuous (i.e., not fixed to whole numbers). That's helpful. We can cross out things like a binomial or Poisson distribution. We also probably expect that scavengers are likely to consume a little bit more or a little be less than our expectations (i.e., the deviations are likely symmetrical rather than stretched high). That means skewed distributions, like an exponential, wouldn't seem very good.

I would think that these slight variations are probably normally distributed. Unless we have a good reason to think otherwise, this is usually a reasonable starting point. And again, we can revisit this. 

So, we have our mean or expected value, $\mu_i$, but we need to describe some variation from this expectation. As a start, let's say $\sigma = 1$. 
```{r}
obs <- rnorm(n, mean = mu, sd = 1)

plot(obs ~ carc, col = "red") # observations
lines(mu ~ carc) # "expected" relationships
abline(a=0, b=1, lty=2) # a 1:1 line just for context
```

That is a fair bit of variation! Moreover, some values are now going _negative_! That won't work! 

So what are our options? Well, we could turn down the noise (standard deviation or $\sigma$) to ensure that no observations are less than zero.  We could also just set every observations that is less than zero to zero. However, both of these strategies seem like we are imposing arbitrary limits on the variability in observations. Plus, with _ad hoc_ approaches like the second one, coding things gets hard (especially in the language of our Bayesian models)^[We _can_ do these things, but they're beyond our current scope.] A better option would be to use a distribution of observation that ensured positive values. 

One that comes readily to mind is the lognormal. Indeed, it is just a normal distribution that is exponentiated. Let's see what that looks like:
```{r}
# Normal distribution
hist(rnorm(1e4), breaks = 50)
# exponentiated normal distribution
hist(exp(rnorm(1e4)), breaks = 50, border = "blue", col = NA)
# the lognormal distribution is the same as the exponentiated normal
hist(rlnorm(1e4), breaks = 50, border = "red", col = NA, add = T)
```
See? 


Note, however, that converting between the standard deviation of the normal and lognormal is a bit weird. A standard deviation of 1 on the normal, linear scale, we would imply a standard deviation of $\log(1) = 0$ on the log scale, but that would mean no deviation at all! Anyway, this is why we simulate, so we can get a sense of what these numbers imply 

```{r}
obs <- rlnorm(n, meanlog = log(mu), sdlog =1/2)

plot(obs ~ carc, col = "red") # observations
lines(mu ~ carc) # predicted line
abline(a=0, b=1, lty=2) # a 1:1 line just for context

```

So that's a bit better, but perhaps too much noise. Let's tone it down a bit.

```{r}
obs <- rlnorm(n, meanlog = log(mu), sdlog =1/3)

plot(obs ~ carc, col = "red") # observations
lines(mu ~ carc) # predicted line
abline(a=0, b=1, lty=2) # a 1:1 line just for context
points(obs ~ carc, col = "blue") # new observations
```

Any points above the 1:1 line might be interpreted as measurement error on the initial or final carcass measurements. It's probably not worth getting too worked about these issues^[If we wanted to enforce our consumed carcass values to be $\leq$ the initial carcass values we could model the _proportion_ of the carcass consumed and use a beta distribution to describe the variation.]; we have data that are fairly realistic and avoid real issues like negative values. Yes, a lognormal does allow for occasional observations that are much larger than our expectation and almost never allows for observations that are much smaller than our expectation (i.e., it's not symmetric), but we're probably in the realm of good enough. 

## And so...? What do we do with this?

Well, congratulations! We've simulated data! This is one example of data we could get from our experimental design. But now that we have the code, shouldn't we futz with things a bit? Absolutely!

One of the first things I would do is just re-run the code. It's easier if we put it all in one place

```{r}
a <- 1/2 # attack rate
n <- 18 # number of aquaria
mean_c <- 1 # 1 g
n_carc <- c( rep(1, 5), rep(2, 5), rep(5, 3), rep(8, 3), rep(10, 2) ) # number of carcasses in each aquarium
carc <- rnorm(n=18, mean = mean_c*n_carc, sd = 0.25) # carcass weights in each aquarium
mu <- a * carc
obs <- rlnorm(n, meanlog = log(mu), sdlog =1/3)

plot(obs ~ carc, col = "red") # observations
lines(mu ~ carc) # predicted line
abline(a=0, b=1, lty=2) # a 1:1 line just for context
```

And then we can run it over and over and over again. Notice that we get different data and plots each time, but none of the parameters (e.g., `a` or `n`) have changed. This variation is simply due to sampling noise. 

What if we changed the attack rate or reduced the `sdlog` to imply less variation in scavenging? Go ahead and try it. In this simple case its relatively easy to understand (and even predict) how things will change, but that's not always the case. Sometimes you even surprise yourself!

Note that if you end up doing a lot of this futzing, it can be helpful to create a function that takes the parameter choices, simulates a data set, and then spits it out or plots it or something (e.g., `sim_dat <- function(a, n_carc, mean_c, sdlog){...}`). You can do this in an organized way to, say, loop over different attack rates to get a better sense of how important that parameter is for your results. But we'll get there...

What else can we do with simulated data? 

*  test our statistics on them^[We will be doing a lot of this under the guise of prior predictive simulation, posterior prediction, and just developing our models so that we know they work OK.]. Do we estimate the True attack rate well? Is it biased? Variable? 
*  see if we can differentiate meaningful from less meaningful values (e.g., of attack rates)
*  compare alternate versions of how the data were simulated (e.g., compare Type I and Type II models) 
*  tweak our study design. What if we added more large groups of carcasses? Would we get a better estimate? Would it help us differentiate the two models?
*  extrapolate to scales or settings we care about (e.g., if things look like this here, what about in whole ponds?)

## Can we recover the attack rate?

Before moving on, let me just illustrate how we might accomplish that first bulleted item. In outline, we want to simulate a data set, fit a model to those data (here it will just be a simple linear model without an intercept), extract the estimate of the attack rate (the slope), and the repeat a bunch of times. We can then see if we are usually close to the True value, whether they estimates tend to be high or low, or any other sorts of things. 

Also, for fun, imagine we wanted to do this with a bunch of different values of the attack rate to see if, for instance, we are good at recovering the True rate when it is high, but not at low values. 

The first step is to set up a data frame (or you could use a matrix) with a column for the value of the attack rate and a row for each simulation. Let's use 1000 simulations for each value of `a`. 

```{r}
sim_results <- data.frame(a_tru = rep(c(0.1, 0.2, 0.3, 0.4, 0.5), each = 1e3), 
                          a_est = -999, # to hold the point estimate of a; using a placeholder value of -999
                          a_sd = -999  # to hold the stdev of our estimate of a
)
```

Then we just want to loop over the 5000 rows of our data frame, simulating a data set, fitting a linear regression line to it (we'll just use canned R code for this, but we can do this in a Bayesian way, later), and then extracting the slope and standard error of the slope, putting them in the right slots in our data frame.
```{r}
# the standard values of key parameters
n <- 18 # number of aquaria
mean_c <- 1 # 1 g
n_carc <- c( rep(1, 5), rep(2, 5), rep(5, 3), rep(8, 3), rep(10, 2) ) # number of carcasses in each aquarium

for(i in 1:nrow(sim_results)){
  # make a simulated data set
  carc <- rnorm(n=18, mean = mean_c*n_carc, sd = 0.25) # carcass weights in each aquarium
  mu <- sim_results$a_tru[i] * carc
  obs <- rlnorm(n, meanlog = log(mu), sdlog =1/3)
  
  # fit a model
  m <- lm(obs ~ carc - 1) # the "-1" means no intercept
  
  # put things in the right slots
  sim_results$a_est[i] <- summary(m)[["coefficients"]]["carc", "Estimate"]
  sim_results$a_sd[i] <- summary(m)[["coefficients"]]["carc", "Std. Error"]
}
```

You should then have a data frame with the true value of the attack rate, the estimate and its standard error. 
```{r}
head(sim_results)
```

We can then do all sorts of things with this. For instance, we might just want want to know if we get the right value, on average:
```{r}
tapply(X = sim_results$a_est, 
       INDEX = sim_results$a_tru, 
       FUN = mean)
```

Here you will see that we tend to be over estimating the attack rate (by about 10\% on average) at each True value of attack rate. First, isn't it good to _know_ that we have that bias?! If these were real data you would never know! Second, you might, with some thought, realize that we didn't model the data the way we simulated them. Our simulated data come from a lognormal distribution, but our linear regression assumes normally distributed observations. The occasional observation of very high carcass consumption tends to bias our estimates up. Had we used a lognormal distribution, I think we would have been fine. Usually you'd want to explore and fix such problems, but here I'm aiming for a simple example. 

We might also want to see if, say, ~95% of our estimates are within $\pm$ two standard deviations of the true value (i.e., that our estimates are well-calibrated). We could do that here, with this sort of code:
```{r}
sim_results$within2 <- ifelse( abs(sim_results$a_est-sim_results$a_tru) <= sim_results$a_sd*2, 1, 0)

tapply(X = sim_results$within2,
       INDEX = sim_results$a_tru, 
       FUN = function(x) sum(x)/length(x))
```
Again, we're seeing this estimate is poorly calibrated, probably for the same reason the mean of the estimates is off. The point is just to see how we could approach thinking about how well or methods were working on data where we _know_ the Truth. 

## Writing it down

We have created a _generative_ model, meaning it can generate new data^[We will see that generative models are essentially Bayesian models, so this was even handier than you might have thought!]. It can be super helpful to write down a model like this in a concise, pretty, perhaps even mathy way. There are, of course, many ways, but we'll follow a set of conventions that are common and also used by McElreath. 

$$
\begin{align}
\text{mass}_i &\sim \text{Lognormal}(\log(\mu_i), \log(\sigma)) \\
\mu_i &= a \times c_i \\
a &= 0.5 \\
\log(\sigma) &= 0.333
\end{align}
$$
Starting with the first line, this means that the remaining mass observed in aquarium $i$ is distributed (that's what the $\sim$ means) as a lognormal with a mean on the the log scale of $\log(\mu_i)$ and a standard deviation on the log scale of $\log(\sigma)$. The corresponds to the `rlnorm(n, meanlog = log(mu), sdlog =1/3)` statement in our code. 
Next we specify the relationship between the amount of carcass mass available ($c_i$) and the expected amount consumed ($\mu_i$). Note that the expectation is not distributed; it is simply equal to $a \times c_i$. This line is often more complicated, but is frequently a linear model like we see here. In our code this was `mu <- a * carc`. 

Finally, the last two lines tell us what values the two parameters, $a$ and $\sigma$, can take. Here they are both single constants, but we might instead allow them to vary, with some constraints. For instance, we might simulate $a$ as coming from a normal distribution with a mean of 0.5 and and a standard deviation of 0.1 (`a <- rnorm(n=1, mean=0.5, sd=0.1)`) in which case we would write it as $a \sim \text{Normal}(0.5, 0.1)$. Notice that we constrained the value of $\sigma$ to be 0.5 _on the natural logarithmic scale_. We could also have just said that $\sigma = \exp(0.5) = 1.648721$, but this is sometime simpler to understand. 

Anyway, this is a nice, compact, transparent way to write out a generative model^[And, as you will see, our Bayesian models, in which case each line has a pretty straightforward meaning: the likelihood, the linear model, and the priors.], one that we'll use a lot in this class. 


# Simulating data from the Type II functional response
 
 Again, our type II function response is of the form:
$$
\mu_i = \frac{a \times c_i}{1+a \times c_i \times h},
$$

where $h$ represents handling time, but everything else is the same. Indeed, we are really only changing the relationship (part 2, above), so this should be quick. Indeed, the full model can be written down as:

$$
\begin{align}
\text{mass}_i &\sim \text{Lognormal}(\log(\mu_i), \log(\sigma)) \\
\mu_i &= \frac{a c_i}{1+a c_i h} \\
a &= 0.5 \\
\log(\sigma) &= 0.5 \\
h &= ?
\end{align}
$$

All we have to do is sort out what $h$ should be. We can constrain handling time to be $\geq 0$ from first principles, but I think we can also say that handling time is also unlikely to be super high, as in it is unlikely that handling time to consume a carcass is on the order of the time of the experiment (48h). We can therefore assume $1 > h > 0$.

Let's try simulating some values. I'm going to use the `curve` function, which is sort of like `abline()` except that it plots an arbitrary function instead of just a line.
```{r}
# attack rate is 0.5, 
# handling time is 0.2 (20% of the duration of the experiment)
# x is the initial carcass mass
curve(0.5*x/(1+0.5*x*0.2), from = 0, to = 10)
abline(a=0, b=1, lty=2)
```

It seems that this line curves away from the 1:1 line a bit, but not super fast. Play wit this a bit to get a sense of how handling time affects the shape of the curve. I think you'll see that as $h \rightarrow 0$ this line becomes more and more like the Type I functional response. 

If we're happy with a value of h = 0.2, and I guess I am, we can then use this relationship to generate expected values for any given initial carcass mass, like we did before with the Type I functional response. I'm going to do it all in one simple block of code, so make sure you see what's changed. 


```{r}
a <- 1/2 # attack rate
h <- 0.2 # handling time
n <- 18 # number of aquaria
mean_c <- 1 # 1 g
n_carc <- c( rep(1, 5), rep(2, 5), rep(5, 3), rep(8, 3), rep(10, 2) ) # number of carcasses in each aquarium
carc <- rnorm(n=18, mean = mean_c*n_carc, sd = 0.25) # carcass weights in each aquarium
mu <- a*carc/(1 + a*carc*h)
obs <- rlnorm(n, meanlog = log(mu), sdlog =1/3)

plot(obs ~ carc, col = "blue") # observations
curve(a*x, col = "red", add = T) # expectation for Type I
curve(a*x/(1 + a*x*h), col = "blue", add = T) # expectation for Type II
abline(a=0, b=1, lty=2) # a 1:1 line just for context
```

Note that I also added the expectation lines for the type I response (red), assuming no handling time, and the type II response (blue) so we can better see how at least the expectations deviate. You might sort of eyeball it and see if you think curves fit to these data could differentiate between the two models. Indeed, those would be next steps. But for now, I think we're done. 

